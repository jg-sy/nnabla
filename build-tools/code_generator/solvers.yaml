Sgd:
  snake_name: sgd
  doc: |
    Stochastic gradient descent (SGD) optimizer.

    .. math::
        w_{t+1} \leftarrow w_t - \eta \Delta w_t

  arguments:
    lr:
      type: float
      default: 1e-3
      doc: Learning rate (:math:`\eta`).

SgdW:
  snake_name: sgdw
  doc: |
    Momentum stochastic gradient descent (SGD) optimizer with decoupled weight decay.

    .. math::
        m_{t} &\leftarrow \gamma m_{t-1} + \eta_t \alpha g_t\\
        w_{t} &\leftarrow w_{t-1} - m_{t} - \eta_t \lambda w_{t-1}

    where :math:`g_t` denotes a gradient,
    :math:`m_t` is momentum of the gradient initialized with 0 at :math:`t=0`,
    :math:`\eta _t` is the scheduled learning rate,
    :math:`\lambda` is the decoupled weight decay rate set by :py:meth:`~nnabla.solver.Solver.weight_decay` method (lazy evaluation),
    and the rest is described in the argument documentation.

  references:
    - |
      `Loshchilov and Hutter, Decoupled Weight Decay Regularization.
      <https://arxiv.org/abs/1711.05101>`_

  arguments:
    lr:
      type: float
      default: 1e-3
      doc:
        Initial learning rate (:math:`\alpha`). Note that you have to manage the scheduled
        learning rate :math:`\eta_t` yourelf. By denoting learning rate updated at the
        :py:meth:`~nnabla.solver.Solver.set_learning_rate`  by :math:`\alpha_t`,
        we define :math:`\eta_t = \frac{\alpha_t}{\alpha}`.
    momentum:
      type: float
      default: 0.9
      doc: Decay rate of momentum (:math:`\gamma`).
    wd:
      type: float
      default: 1e-4
      doc:
        The default weight decay rate (:math:`\lambda`). The weight decay operation is fused into the
        update operation in SgdW. It uses this default decay rate unless you overwrite a decay rate
        via :py:meth:`~nnabla.solver.Solver.weight_decay` for the next call of :py:meth:`~nnabla.solver.Solver.update`.

Momentum:
  snake_name: momentum
  doc: |
    SGD with Momentum.

    .. math::
        v_t &\leftarrow \gamma v_{t-1} + \eta \Delta w_t\\
        w_{t+1} &\leftarrow w_t - v_t

  references:
    - |
      `Ning Qian : On the Momentum Term in Gradient Descent Learning Algorithms.
      <http://www.columbia.edu/~nq6/publications/momentum.pdf>`_

  arguments:
    lr:
      type: float
      default: 1e-3
      doc: Initial learning rate (:math:`\eta_0`).
    momentum:
      type: float
      default: 0.9
      doc: Decay rate of momentum.

Lars:
  snake_name: lars
  doc: |
    LARS with Momentum.

    .. math::
        \lambda &= \eta \frac{\| w_t \|}{\| g_t \| + \| \beta w_t \|} \\
        v_{t+1} &\leftarrow m v_t + \gamma_t \lambda (g_t + \beta w_t) \\
        w_{t+1} &\leftarrow w_t - v_{t+1}

    where :math:`g_t` denotes a gradient,
    :math:`\beta` is the decoupled weight decay rate set by :py:meth:`~nnabla.solver.Solver.weight_decay` method (lazy evaluation),
    :math:`v_0 \leftarrow 0`, and the rest is described in the argument documentation.

  references:
    - |
      `Yang You, Igor Gitman, Boris Ginsburg
      Large Batch Training of Convolutional Networks
      <https://arxiv.org/abs/1708.03888>`_

  arguments:
    lr:
      type: float
      default: 1e-3
      doc: Learning rate (:math:`\gamma_t`).
    momentum:
      type: float
      default: 0.9
      doc: Decay rate of momentum (:math:`m`).
    coefficient:
      type: float
      default: 0.001
      doc: Trust coefficient (:math:`\eta`).
    eps:
      type: float
      default: 1e-6
      doc: Small value for avoiding zero devision(:math:`\epsilon`).

Nesterov:
  snake_name: nesterov
  doc: |
    Nesterov Accelerated Gradient optimizer.

    .. math::
        v_t &\leftarrow \gamma v_{t-1} - \eta \Delta w_t\\
        w_{t+1} &\leftarrow w_t - \gamma v_{t-1} + \left(1 + \gamma \right) v_t

  references:
    - |
      Yurii Nesterov. A method for unconstrained convex minimization problem with
      the rate of convergence :math:`o(1/k2)`.

  arguments:
    lr:
      type: float
      default: 1e-3
      doc: Learning rate (:math:`\eta`).
    momentum:
      type: float
      default: 0.9
      doc: Decay rate of momentum.

Adadelta:
  snake_name: adadelta
  doc: |
    AdaDelta optimizer.

    .. math::
        g_t &\leftarrow \Delta w_t\\
        v_t &\leftarrow - \frac{RMS \left[ v_t \right]_{t-1}}
            {RMS \left[ g \right]_t}g_t\\
        w_{t+1} &\leftarrow w_t + \eta v_t

  references:
    - |
      `Matthew D. Zeiler.
      ADADELTA: An Adaptive Learning Rate Method.
      <https://arxiv.org/abs/1212.5701>`_

  arguments:
    lr:
      type: float
      default: 1.0
      doc: Learning rate (:math:`\eta`).
    decay:
      type: float
      default: 0.95
      doc: Decay rate (:math:`\gamma`).
    eps:
      type: float
      default: 1e-6
      doc: Small value for avoiding zero division(:math:`\epsilon`).

Adagrad:
  snake_name: adagrad
  doc: |
    ADAGrad optimizer.

    .. math::
        g_t &\leftarrow \Delta w_t\\
        G_t &\leftarrow G_{t-1} + g_t^2\\
        w_{t+1} &\leftarrow w_t - \frac{\eta}{\sqrt{G_t} + \epsilon} g_t

  references:
    - |
      `John Duchi, Elad Hazan and Yoram Singer (2011).
      Adaptive Subgradient Methods for Online Learning and Stochastic Optimization.
      <http://www.jmlr.org/papers/volume12/duchi11a/duchi11a.pdf>`_

  arguments:
    lr:
      type: float
      default: 1e-2
      doc: Learning rate (:math:`\eta`).
    eps:
      type: float
      default: 1e-8
      doc: Small value for avoiding zero division(:math:`\epsilon`).

AdaBelief:
  snake_name: adabelief
  doc: |
    AdaBelief optimizer.

    .. math::
        m_t &\leftarrow \beta_1 m_{t-1} + (1 - \beta_1) g_t\\
        s_t &\leftarrow \beta_2 s_{t-1} + (1 - \beta_2) (g_t - m_t)^2\\
        w_{t+1} &\leftarrow w_t - \alpha
            \frac{\sqrt{1 - \beta_2^t}}{1 - \beta_1^t}
            \frac{m_t}{\sqrt{s_t + \epsilon} + \epsilon}

  references:
    - |
      `Juntang Zhuang, et al. (2020).
      AdaBelief Optimizer: Adapting Stepsizes by the Belief in Observed Gradients.
      <https://arxiv.org/pdf/2010.07468.pdf>`_

  arguments:
    alpha:
      type: float
      default: 1e-3
      doc: Step size (:math:`\alpha`).
    beta1:
      type: float
      default: 0.9
      doc: Decay rate of first-order momentum (:math:`\beta_1`).
    beta2:
      type: float
      default: 0.999
      doc: Decay rate of second-order momentum (:math:`\beta_2`).
    eps:
      type: float
      default: 1e-8
      doc: Small value for avoiding zero division(:math:`\epsilon`).
    wd:
      type: float
      default: 0.0
      doc:
        The default weight decay rate enabled only when weight_decouple is true.
        If enabled, the weight decay operation is decoupled and fused into the update operation.
        It uses this default decay rate unless you overwrite a decay rate
        via :py:meth:`~nnabla.solver.Solver.weight_decay` for the next call of :py:meth:`~nnabla.solver.Solver.update`.
    amsgrad:
      type: bool
      default: False
      doc: Perform AMSGrad variant of AdaBelief.
    weight_decouple:
      type: bool
      default: False
      doc: Whether to perform decoupled weight decay as in AdamW.
    fixed_decay:
      type: bool
      default: False
      doc: If True, the weight decay ratio will be kept fixed. Note that this option only takes effect when weight_decouple option is enabled.
    rectify:
      type: bool
      default: False
      doc: Perform RAdam variant of AdaBelief.

RMSprop:
  snake_name: rmsprop
  doc: |
    RMSprop optimizer (Geoffery Hinton).

    .. math::
        g_t &\leftarrow \Delta w_t\\
        v_t &\leftarrow \gamma v_{t-1} + \left(1 - \gamma \right) g_t^2\\
        w_{t+1} &\leftarrow w_t - \eta \frac{g_t}{\sqrt{v_t} + \epsilon}

  references:
    - |
      `Geoff Hinton.
      Lecture 6a : Overview of mini-batch gradient descent.
      <http://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf>`_

  arguments:
    lr:
      type: float
      default: 1e-3
      doc: Learning rate (:math:`\eta`).
    decay:
      type: float
      default: 0.9
      doc: Decay rate (:math:`\gamma`).
    eps:
      type: float
      default: 1e-8
      doc: Small value for avoiding zero division(:math:`\epsilon`).

RMSpropGraves:
  snake_name: rmsprop_graves
  doc: |
    RMSpropGraves optimizer (Alex Graves).

    .. math::
        n_t &\leftarrow \rho n_{t-1} + \left(1 - \rho \right) {e_t}^2\\
        g_t &\leftarrow \rho g_{t-1} + \left(1 - \rho \right) e_t\\
        d_t &\leftarrow \beta d_{t-1} - \eta \frac{e_t}{\sqrt{n_t - {g_t}^2 + \epsilon}}\\
        w_{t+1} &\leftarrow w_t + d_t

    where :math:`e_t` denotes the gradient.

  references:
    - |
      `Alex Graves.
      Generating Sequences With Recurrent Neural Networks.
      <http://arxiv.org/pdf/1308.0850v5.pdf>`_

  arguments:
    lr:
      type: float
      default: 1e-4
      doc: Learning rate (:math:`\eta`).
    decay:
      type: float
      default: 0.95
      doc: Decay rate (:math:`\rho`).
    momentum:
      type: float
      default: 0.9
      doc: Momentum (:math:`\beta`)
    eps:
      type: float
      default: 1e-4
      doc: Small value for avoiding zero division(:math:`\epsilon`).

Adam:
  snake_name: adam
  doc: |
    ADAM optimizer.

    .. math::
        m_t &\leftarrow \beta_1 m_{t-1} + (1 - \beta_1) g_t\\
        v_t &\leftarrow \beta_2 v_{t-1} + (1 - \beta_2) g_t^2\\
        w_{t+1} &\leftarrow w_t - \alpha
            \frac{\sqrt{1 - \beta_2^t}}{1 - \beta_1^t}
            \frac{m_t}{\sqrt{v_t} + \epsilon}

    where :math:`g_t` denotes a gradient, and let :math:`m_0 \leftarrow 0`
    and :math:`v_0 \leftarrow 0`.

  references:
    - |
      `Kingma and Ba, Adam: A Method for Stochastic Optimization.
      <https://arxiv.org/abs/1412.6980>`_

  arguments:
    alpha:
      type: float
      default: 1e-3
      doc: Step size (:math:`\alpha`).
    beta1:
      type: float
      default: 0.9
      doc: Decay rate of first-order momentum (:math:`\beta_1`).
    beta2:
      type: float
      default: 0.999
      doc: Decay rate of second-order momentum (:math:`\beta_2`).
    eps:
      type: float
      default: 1e-8
      doc: Small value for avoiding zero division(:math:`\epsilon`).

AdamW:
  snake_name: adamw
  doc: |
    ADAM optimizer with decoupled weight decay.

    .. math::
        m_t &\leftarrow \beta_1 m_{t-1} + (1 - \beta_1) g_t\\
        v_t &\leftarrow \beta_2 v_{t-1} + (1 - \beta_2) g_t^2\\
        \hat{m} &= m_t / (1-\beta_1^t)\\
        \hat{v} &= v_t / (1-\beta_2^t)\\
        w_{t} &\leftarrow w_{t-1}
            - \eta_t \left(
                \alpha \frac{\hat{m}}{\left(\sqrt{\hat{v}} + \epsilon \right)}
                + \lambda w_{t-1} \right)

    where :math:`g_t` denotes a gradient,
    :math:`m_t` and :math:`v_t` are 1st and 2nd order momentum of the gradient initialized with 0 at :math:`t=0`,
    :math:`\eta _t` is the scheduled learning rate,
    :math:`\lambda` is the decoupled weight decay rate set by :py:meth:`~nnabla.solver.Solver.weight_decay` method (lazy evaluation),
    and the rest is described in the argument documentation.

  references:
    - |
      `Loshchilov and Hutter, Decoupled Weight Decay Regularization.
      <https://arxiv.org/abs/1711.05101>`_

  arguments:
    alpha:
      type: float
      default: 1e-3
      doc:
        Initial learning rate (:math:`\alpha`). Note that you have to manage the scheduled
        learning rate :math:`\eta_t` yourelf. By denoting learning rate updated at the
        :py:meth:`~nnabla.solver.Solver.set_learning_rate`  by :math:`\alpha_t`,
        we define :math:`\eta_t = \frac{\alpha_t}{\alpha}`.

    beta1:
      type: float
      default: 0.9
      doc: Decay rate of first-order momentum (:math:`\beta_1`).
    beta2:
      type: float
      default: 0.999
      doc: Decay rate of second-order momentum (:math:`\beta_2`).
    eps:
      type: float
      default: 1e-8
      doc: Small value for avoiding zero division(:math:`\epsilon`).
    wd:
      type: float
      default: 1e-4
      doc:
        The default weight decay rate (:math:`\lambda`). The weight decay operation is fused into the
        update operation in this solver. It uses this default decay rate unless you overwrite a decay rate
        via :py:meth:`~nnabla.solver.Solver.weight_decay` for the next call of :py:meth:`~nnabla.solver.Solver.update`.

AdaBound:
  snake_name: adabound
  doc: |
    AdaBound optimizer applies dynamic bounds on learning rates to Adam.

    .. math::
        w_{t+1} &\leftarrow w_t - \eta_t*m_t\\
        \eta_t &= clip( \alpha\frac{\sqrt{1 - \beta_2^t}}{(1 - \beta_1^t)(\sqrt{v_t} + \epsilon)}, \eta_l(t), \eta_u(t))\\
        \eta_l(t) &= (1 - (1/((1-\gamma)t+1)))\alpha^*\\
        \eta_u(t) &= (1 + (1/((1-\gamma)t)))\alpha^*

    where :math:`\alpha^*` (``final_lr``) is scaled by a factor defined as the current value of :math:`\alpha` (set by ``set_learning_rate(lr)``) over initial value of :math:`\alpha`, so that learnign rate scheduling is properly applied to both :math:`\alpha` and :math:`\alpha^*`.

  references:
    - |
      `L. Luo, Y. Xiong, Y. Liu and X. Sun. Adaptive Gradient Methods with Dynamic Bound of Learning Rate.
      <https://arxiv.org/abs/1902.09843>`_

  arguments:
    alpha:
      type: float
      default: 1e-3
      doc: Step size (:math:`\alpha`).
    beta1:
      type: float
      default: 0.9
      doc: Decay rate of first-order momentum (:math:`\beta_1`).
    beta2:
      type: float
      default: 0.999
      doc: Decay rate of second-order momentum (:math:`\beta_2`).
    eps:
      type: float
      default: 1e-8
      doc: Small value for avoiding zero division(:math:`\epsilon`).
    final_lr:
      type: float
      default: 0.1
      doc: Final (SGD) learning rate.
    gamma:
      type: float
      default: 0.001
      doc: Convergence speed of the bound functions.
Adamax:
  snake_name: adamax
  doc: |
    ADAMAX Optimizer.

    .. math::
        m_t &\leftarrow \beta_1 m_{t-1} + (1 - \beta_1) g_t\\
        v_t &\leftarrow \max\left(\beta_2 v_{t-1}, |g_t|\right)\\
        w_{t+1} &\leftarrow w_t - \alpha
            \frac{\sqrt{1 - \beta_2^t}}{1 - \beta_1^t}
            \frac{m_t}{v_t + \epsilon}

    where :math:`g_t` denotes a gradient, and let :math:`m_0 \leftarrow 0`
    and :math:`v_0 \leftarrow 0`, :math:`v_t` is an
    exponentially weighted infinity norm of a sequence of
    gradients :math:`t=0,...,t`.

  references:
    - |
      `Kingma and Ba, Adam: A Method for Stochastic Optimization.
      <https://arxiv.org/abs/1412.6980>`_

  arguments:
    alpha:
      type: float
      default: 2e-3
      doc: Step size (:math:`\alpha`).
    beta1:
      type: float
      default: 0.9
      doc: Decay rate of first-order momentum (:math:`\beta_1`).
    beta2:
      type: float
      default: 0.999
      doc: Decay rate of inf-order momentum (:math:`\beta_2`).
    eps:
      type: float
      default: 1e-8
      doc: Small value for avoiding zero division(:math:`\epsilon`).

AMSGRAD:
  snake_name: amsgrad
  doc: |
    AMSGRAD optimizer.

    .. math::
        m_t &\leftarrow \beta_1 m_{t-1} + (1 - \beta_1) g_t\\
        v_t &\leftarrow \beta_2 v_{t-1} + (1 - \beta_2) g_t^2\\
        \hat{v_t} &= \max(\hat{v_{t-1}}, v_t)\\
        w_{t+1} &\leftarrow w_t - \alpha
            \frac{m_t}{\sqrt{\hat{v_t}} + \epsilon}

    where :math:`g_t` denotes a gradient, and let :math:`m_0 \leftarrow 0`
    and :math:`v_0 \leftarrow 0`.

  references:
    - |
      `Reddi et al. On the convergence of ADAM and beyond.
      <https://openreview.net/pdf?id=ryQu7f-RZ>`_

  arguments:
    alpha:
      type: float
      default: 1e-3
      doc: Step size (:math:`\alpha`).
    beta1:
      type: float
      default: 0.9
      doc: Decay rate of first-order momentum (:math:`\beta_1`).
    beta2:
      type: float
      default: 0.999
      doc: Decay rate of second-order momentum (:math:`\beta_2`).
    eps:
      type: float
      default: 1e-8
      doc: Small value for avoiding zero division(:math:`\epsilon`). Note this does not appear in the paper.
    bias_correction:
      type: bool
      default: False
      doc: Apply bias correction to moving averages defined in ADAM. Note this does not appear in the paper.

AMSBound:
  snake_name: amsbound
  doc: |
    AMSBound optimizer applies dynamic bounds on learning rates to AMSGrad.

    .. math::
        w_{t+1} &\leftarrow w_t - \eta_t*m_t\\
        \eta_t &= clip( \alpha\frac{\sqrt{1 - \beta_2^t}}{(1 - \beta_1^t)(\sqrt{\hat{v_t}} + \epsilon)}, \eta_l(t), \eta_u(t))\\
        \hat{v_t} &= \max(\hat{v_{t-1}}, v_t)\\
        \eta_l(t) &= (1 - (1/((1-\gamma)t+1)))\alpha^*\\
        \eta_u(t) &= (1 + (1/((1-\gamma)t)))\alpha^*

    where :math:`\alpha^*` (``final_lr``) is scaled by a factor defined as the current value of :math:`\alpha` (set by ``set_learning_rate(lr)``) over initial value of :math:`\alpha`, so that learnign rate scheduling is properly applied to both :math:`\alpha` and :math:`\alpha^*`.

  references:
    - |
      `L. Luo, Y. Xiong, Y. Liu and X. Sun. Adaptive Gradient Methods with Dynamic Bound of Learning Rate.
      <https://arxiv.org/abs/1902.09843>`_

  arguments:
    alpha:
      type: float
      default: 1e-3
      doc: Step size (:math:`\alpha`).
    beta1:
      type: float
      default: 0.9
      doc: Decay rate of first-order momentum (:math:`\beta_1`).
    beta2:
      type: float
      default: 0.999
      doc: Decay rate of second-order momentum (:math:`\beta_2`).
    eps:
      type: float
      default: 1e-8
      doc: Small value for avoiding zero division(:math:`\epsilon`). Note this does not appear in the paper.
    final_lr:
      type: float
      default: 0.1
      doc: Final (SGD) learning rtae
    gamma:
      type: float
      default: 0.001
      doc: Convergence speed of the bound functions
    bias_correction:
      type: bool
      default: False
      doc: Apply bias correction to moving averages defined in ADAM. Note this does not appear in the paper.


Lamb:
  snake_name: lamb
  doc: |
    LAMB optimizer.

    .. math::
        m_t &\leftarrow \beta_1 m_{t-1} + (1 - \beta_1) g_t\\
        v_t &\leftarrow \beta_2 v_{t-1} + (1 - \beta_2) g_t^2\\
        \hat{m} &= m_t / (1-\beta_1^t)\\
        \hat{v} &= v_t / (1-\beta_2^t)\\
        r &= \frac{\hat{m}}{\sqrt{\hat{v}}+\epsilon}\\
        w_t &\leftarrow w_{t-1} - \eta_t \frac{\phi (\|w_{t-1}\|)}{\|r + \lambda w_{t-1} \|} \left(r + \lambda w_{t-1} \right)

    where :math:`g_t` denotes a gradient,
    :math:`m_t` and :math:`v_t` are 1st and 2nd order momentum of the gradient initialized with 0 at :math:`t=0`,
    :math:`\lambda` is the decoupled weight decay rate set by :py:meth:`~nnabla.solver.Solver.weight_decay` method (lazy evaluation),
    :math:`\phi` is a scaling function defined as :math:`\phi(z)=\min\{\max\{z, \gamma_l\}, \gamma_u\}`,
    and the rest is described in the arguments.

  references:
    - |
      `Yang You, Jing Li, Sashank Reddi. Large Batch Optimization for Deep Learning: Training BERT in 76 minutes.
      <https://arxiv.org/abs/1904.00962>`_

  arguments:
    eta:
      type: float
      doc: Learning rate (:math:`\eta_t`).
      default: 1e-3
    beta1:
      type: float
      doc: Decay rate of first-order momentum (:math:`\beta_1`).
      default: 0.9
    beta2:
      type: float
      doc: Decay rate of second-order momentum (:math:`\beta_2`).
      default: 0.999
    gamma_l:
      type: float
      doc: Lower bound of the clamp scaling function :math:`\phi` (:math:`\gamma_l`).
      default: 0.0
    gamma_u:
      type: float
      doc: Upper bound the clamp scaling function :math:`\phi` (:math:`\gamma_u`).
      default: 10.0
    eps:
      type: float
      doc: Small value for avoiding zero division (:math:`\epsilon`).
      default: 1e-6
    bias_correction:
      type: bool
      doc: Whether to apply bias correction in momentum computation :math:`\hat{m}` and :math:`\hat{v}`.
      default: False

Lion:
  snake_name: lion
  doc: |
    Lion optimizer.

    .. math::
        u = \beta_1 m_{t} + (1 - \beta_1) g_{t}\\
        u = {\rm sign}(u)\\
        m_{t+1} &\leftarrow \beta_2 m_{t} + (1 - \beta_2) g_{t}\\
        w_{t+1} &\leftarrow w_t - \alpha \left( u + \lambda w_t \right)

    where :math:`g_t` denotes a gradient,
    :math:`m_t` is momentum of the gradient initialized with 0 at :math:`t=0`,
    :math:`\lambda` is the decoupled weight decay rate set by :py:meth:`~nnabla.solver.Solver.weight_decay` method (lazy evaluation),
    and the rest is described in the arguments.

  references:
    - |
      `Xiangning Chen et al., Symbolic Discovery of Optimization Algorithms.
      <https://arxiv.org/abs/2302.06675>`_

  arguments:
    lr:
      type: float
      default: 1e-4
      doc: Step size (:math:`\alpha`).
    beta1:
      type: float
      default: 0.9
      doc: Decay rate (:math:`\beta_1`).
    beta2:
      type: float
      default: 0.99
      doc: Decay rate (:math:`\beta_2`).
